{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.getcwd()[-15:] != 'BachelorProject':\n",
    "    os.chdir('../')\n",
    "\n",
    "from utils.metrics import get_link_prediction_metrics\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "\n",
    "\n",
    "# Import data\n",
    "folder_name = 'DTU_Test/Test_folder'\n",
    "logits = {}\n",
    "labels = {}\n",
    "MRRs = {}\n",
    "\n",
    "for model_folder in os.listdir(folder_name):\n",
    "    if '_' in model_folder:\n",
    "        continue\n",
    "\n",
    "    model_folder_path = f'{folder_name}/{model_folder}'\n",
    "    for data_name in os.listdir(model_folder_path):\n",
    "        if data_name != 'tgbl-flight':\n",
    "            continue\n",
    "        for run_name in os.listdir(f'{model_folder_path}/{data_name}'):\n",
    "            if '.' in run_name:\n",
    "                continue\n",
    "            for file_name in os.listdir(f'{model_folder_path}/{data_name}/{run_name}'):\n",
    "                #print(file_name)\n",
    "                # OBS exception for EdgeBank!\n",
    "\n",
    "                if 'logits' in file_name:\n",
    "                    data = torch.load(f'{model_folder_path}/{data_name}/{run_name}/{file_name}', map_location=torch.device('cpu'))\n",
    "                    if type(data[0]) == torch.Tensor:\n",
    "                        logits[file_name] = data if 'EdgeBank' not in file_name else torch.stack(data).flatten()\n",
    "                    else:\n",
    "                        logits[file_name] = data if 'EdgeBank' not in file_name else torch.Tensor(np.array(data).flatten())\n",
    "                elif 'labels' in file_name:\n",
    "                    data = torch.load(f'{model_folder_path}/{data_name}/{run_name}/{file_name}', map_location=torch.device('cpu'))\n",
    "                    if type(data[0]) == torch.Tensor:\n",
    "                        labels[file_name] = data if 'EdgeBank' not in file_name else torch.stack(data).flatten()\n",
    "                    else:\n",
    "                        labels[file_name] = data if 'EdgeBank' not in file_name else torch.Tensor(np.array(data).flatten())\n",
    "                elif 'all_val_metric' in file_name:\n",
    "                    MRRs[model_folder] = np.load(f'{model_folder_path}/{data_name}/{run_name}/{file_name}')\n",
    "\n",
    "# Find best epoch for each model\n",
    "model_names = ['EdgeBank'] + list(MRRs.keys())\n",
    "best_epochs = {}\n",
    "for model in MRRs:\n",
    "    best_epochs[model] = MRRs[model].argmax()\n",
    "\n",
    "# Training + test data (train: logits from best epoch)\n",
    "training_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for key in logits:\n",
    "    model_name = key.split('_')[0]\n",
    "    if 'train' in key:\n",
    "        training_data[model_name] = logits[key] if model_name=='EdgeBank' else logits[key][best_epochs[model_name]].detach().numpy()\n",
    "    elif 'test' in key:\n",
    "        test_data[model_name] = logits[key] if model_name=='EdgeBank' else logits[key].detach().numpy()\n",
    "\n",
    "# Add labels\n",
    "for key in labels:\n",
    "    if not 'labels' in training_data.keys() and 'train' in key:\n",
    "        training_data['labels'] = labels[key][0]\n",
    "    elif not 'labels' in test_data.keys() and 'test' in key:\n",
    "        test_data['labels'] = labels[key]\n",
    "\n",
    "train_df = pd.DataFrame(training_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in training_data:\n",
    "#     print(key, training_data[key].shape)\n",
    "#     print(key, test_data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are labels the same? YES\n",
    "# for key1 in labels.keys():\n",
    "#     for key2 in labels.keys():\n",
    "#         if key1 == key2 or key1.split('_')[-1] != key2.split('_')[-1]:\n",
    "#             continue\n",
    "\n",
    "#         if not np.array_equal(labels[key1][np.random.randint(len(labels[key1]))], labels[key2][np.random.randint(len(labels[key2]))]):\n",
    "#             print(f'Labels {key1} and {key2} are not equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class imbalance\n",
    "# for file in ['TGN_tgbl-wiki_labels_train.pth', 'TGN_tgbl-wiki_labels_test.pth']:\n",
    "#     data = labels[file] if not 'train' in file else labels[file].T\n",
    "#     print(data.shape)\n",
    "\n",
    "#     print(sum(data == 1))\n",
    "#     print(sum(data == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(name='tgbl-comment')\n",
    "model_performances = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Train\n",
    "    print(f'\\n\\n-------------------- Model {model_name} --------------------')\n",
    "    print('Model alone')\n",
    "    formula = \"labels ~\" + model_name\n",
    "    model = sm.Logit.from_formula(formula, data=train_df)\n",
    "    result = model.fit()\n",
    "    print(result.conf_int())\n",
    "\n",
    "    # Test\n",
    "    probs = result.predict(test_df[model_name])\n",
    "    predicts = (probs > 0.5).astype(int)\n",
    "    hits = predicts == test_df['labels']\n",
    "    # print(np.mean(hits))\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    probs_batches = np.array_split(probs, len(probs)/101)\n",
    "    mrrs = []\n",
    "    for p_batch in probs_batches:\n",
    "        \n",
    "        # Eval metrics\n",
    "        input_dict = {\n",
    "            'y_pred_pos': np.array(p_batch[0]),\n",
    "            'y_pred_neg': np.array(p_batch[1:]),\n",
    "            'eval_metric': ['mrr']\n",
    "        }   \n",
    "\n",
    "        mrrs.append(evaluator.eval(input_dict)['mrr'])\n",
    "\n",
    "    train_perf = get_link_prediction_metrics(torch.Tensor(predicts), torch.Tensor(test_df['labels']))\n",
    "    pr_auc = train_perf['pr_auc']\n",
    "    roc_auc = train_perf['roc_auc']\n",
    "    mrr = float(np.mean(np.array(mrrs)))\n",
    "\n",
    "    print('mrr:', mrr)\n",
    "    print('pr-auc:', pr_auc)\n",
    "    print('roc-auc:', roc_auc)\n",
    "    \n",
    "    model_performances[model_name] = [mrr, pr_auc, roc_auc]\n",
    "\n",
    "    # Train together with EdgeBank\n",
    "    if model_name == 'EdgeBank':\n",
    "        continue\n",
    "    print(f'\\n\\n------------- {model_name} + EdgeBank -------------')\n",
    "    formula = \"labels ~ \" + model_name + ' * EdgeBank'\n",
    "    model = sm.Logit.from_formula(formula, data=train_df)\n",
    "    result = model.fit()\n",
    "    print(result.conf_int())\n",
    "\n",
    "    # Test\n",
    "    probs = result.predict(test_df[[model_name, 'EdgeBank']])\n",
    "    predicts = (probs > 0.5).astype(int)\n",
    "    hits = predicts == test_df['labels']\n",
    "    # print(np.mean(hits))\n",
    "\n",
    "    # Loop over every 101th value of probs\n",
    "    probs = np.array(probs)\n",
    "    probs_batches = np.array_split(probs, len(probs)/101)\n",
    "    mrrs = []\n",
    "    for p_batch in probs_batches:\n",
    "        \n",
    "        # Eval metrics\n",
    "        input_dict = {\n",
    "            'y_pred_pos': np.array(p_batch[0]),\n",
    "            'y_pred_neg': np.array(p_batch[1:]),\n",
    "            'eval_metric': ['mrr']\n",
    "        }   \n",
    "\n",
    "        mrrs.append(evaluator.eval(input_dict)['mrr'])\n",
    "\n",
    "    train_perf = get_link_prediction_metrics(torch.Tensor(predicts), torch.Tensor(test_df['labels']))\n",
    "    pr_auc = train_perf['pr_auc']\n",
    "    roc_auc = train_perf['roc_auc']\n",
    "    mrr = float(np.mean(np.array(mrrs)))\n",
    "\n",
    "    print('mrr:', mrr)\n",
    "    print('pr-auc:', pr_auc)\n",
    "    print('roc-auc:', roc_auc)\n",
    "\n",
    "    model_performances['EdgeBank_' + model_name] = [mrr, pr_auc, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "os.makedirs(f'{folder_name}/frozen_models/', exist_ok=True)\n",
    "for key in model_performances:\n",
    "    torch.save(model_performances[key], f'{folder_name}/frozen_models/{key}.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(name='tgbl-wiki')\n",
    "model_performances = {}\n",
    "\n",
    "# Drop TCL from model names\n",
    "# model_names = list(train_df.columns.drop('labels'))\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Prepare the training data for the meta-model\n",
    "    X_train_meta = train_df[[model_name, 'EdgeBank']].values\n",
    "    y_train_meta = train_df['labels'].values\n",
    "\n",
    "    # Train the meta-model\n",
    "    meta_model = LogisticRegressionCV(cv=10, max_iter=1000)\n",
    "    meta_model.fit(X_train_meta, y_train_meta)\n",
    "\n",
    "    # Prepare the testing data for the meta-model\n",
    "    X_test_meta = test_df[[model_name, 'EdgeBank']].values\n",
    "    y_test_meta = test_df['labels'].values\n",
    "\n",
    "    # Make predictions with the meta-model\n",
    "    probs = meta_model.predict_proba(X_test_meta)[:, 1]\n",
    "    predicts = (probs > 0.5).astype(int)\n",
    "\n",
    "    # Loop over every 101th value of probs\n",
    "    probs = np.array(probs)\n",
    "    probs_batches = np.array_split(probs, len(probs)/101)\n",
    "    mrrs = []\n",
    "    for p_batch in probs_batches:\n",
    "        \n",
    "        # Eval metrics\n",
    "        input_dict = {\n",
    "            'y_pred_pos': np.array(p_batch[0]),\n",
    "            'y_pred_neg': np.array(p_batch[1:]),\n",
    "            'eval_metric': ['mrr']\n",
    "        }   \n",
    "\n",
    "        mrrs.append(evaluator.eval(input_dict)['mrr'])\n",
    "\n",
    "    train_perf = get_link_prediction_metrics(torch.Tensor(predicts), torch.Tensor(test_df['labels']))\n",
    "    pr_auc = train_perf['pr_auc']\n",
    "    roc_auc = train_perf['roc_auc']\n",
    "    mrr = float(np.mean(np.array(mrrs)))\n",
    "\n",
    "    print('model:', model_name, 'EdgeBank')\n",
    "    print('weights:', meta_model.coef_)\n",
    "    print('mrr:', mrr)\n",
    "    print('pr-auc:', pr_auc)\n",
    "    print('roc-auc:', roc_auc)\n",
    "\n",
    "    model_performances['EdgeBank_' + model_name] = [mrr, pr_auc, roc_auc]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_performances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
